{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SCG Informatics Cluster \u00b6 The SCG Informatics Cluster resources are available for use by labs engaged in genetics and bioinformatics research. The SCG Cluster is designed and managed to support large memory, serial and single node threaded applications as well as applications and workflows that require many small files. Other clusters on campus are available for more general-purpose or HPC oriented user communities, see the Stanford Research Computing Center for more details. Quick Links \u00b6 SCG OnDemand SCG Slack Workspace Stanford Research Computing Center User Tips & Tricks \u00b6 SCG Primer How to use rclone with Stanford Box","title":"Home"},{"location":"#scg-informatics-cluster","text":"The SCG Informatics Cluster resources are available for use by labs engaged in genetics and bioinformatics research. The SCG Cluster is designed and managed to support large memory, serial and single node threaded applications as well as applications and workflows that require many small files. Other clusters on campus are available for more general-purpose or HPC oriented user communities, see the Stanford Research Computing Center for more details.","title":"SCG Informatics Cluster"},{"location":"#quick-links","text":"SCG OnDemand SCG Slack Workspace Stanford Research Computing Center","title":"Quick Links"},{"location":"#user-tips-tricks","text":"SCG Primer How to use rclone with Stanford Box","title":"User Tips &amp; Tricks"},{"location":"accounts/","text":"Requirements \u00b6 Using the SCG resources requires a SUNetID which is associated with one or more PI (faculty) lab groups. Collaborators not affiliated with Stanford University may request a sponsored SUNetID by applying on the Stanford University Accounts website and then asking an SCG affiliated PI or project owner to sponsor the request. Basic sponsored SUNet accounts (without email or AFS storage) are free. PI/Lab Affiliation \u00b6 If your lab has no current affiliation with SCG, please send an email to the SCG Action Team to receive information on how to become a member of the SCG cluster. If not the PI of your lab group, please include the PI in the CC of the email. Service Tiers and Rates \u00b6 There are two service tiers for PI/Lab affiliation with SCG: Tier Storage Quota Compute Partition Access Free 128 GB interactive, nih_s10 Full 7 TB or more batch, interactive, nih_s10 There is no charge for the Free Tier access to SCG. For Full Tier access, rates are: $56.00/month $8.00/TB/month for storage over initial 7 TB quota $0.05/core-hour for batch partition usage User Accounts \u00b6 Once a SUNetID is acquired, an SCG cluster account can be requested by sending an email to the SCG Action Team from the PI for the lab group or from the person requesting the account with the PI cc\u2019d on the email. Projects \u00b6 For projects that span multiple PIs, have dedicated project funding, or other special requirements which don\u2019t fit well within a PI associated lab account, a special project can be created. Projects are set up on a case by case basis around the specific needs and requirements for each one. Mail the SCG Action Team to request setting up a project.","title":"Requesting Accounts"},{"location":"accounts/#requirements","text":"Using the SCG resources requires a SUNetID which is associated with one or more PI (faculty) lab groups. Collaborators not affiliated with Stanford University may request a sponsored SUNetID by applying on the Stanford University Accounts website and then asking an SCG affiliated PI or project owner to sponsor the request. Basic sponsored SUNet accounts (without email or AFS storage) are free.","title":"Requirements"},{"location":"accounts/#pilab-affiliation","text":"If your lab has no current affiliation with SCG, please send an email to the SCG Action Team to receive information on how to become a member of the SCG cluster. If not the PI of your lab group, please include the PI in the CC of the email.","title":"PI/Lab Affiliation"},{"location":"accounts/#service-tiers-and-rates","text":"There are two service tiers for PI/Lab affiliation with SCG: Tier Storage Quota Compute Partition Access Free 128 GB interactive, nih_s10 Full 7 TB or more batch, interactive, nih_s10 There is no charge for the Free Tier access to SCG. For Full Tier access, rates are: $56.00/month $8.00/TB/month for storage over initial 7 TB quota $0.05/core-hour for batch partition usage","title":"Service Tiers and Rates"},{"location":"accounts/#user-accounts","text":"Once a SUNetID is acquired, an SCG cluster account can be requested by sending an email to the SCG Action Team from the PI for the lab group or from the person requesting the account with the PI cc\u2019d on the email.","title":"User Accounts"},{"location":"accounts/#projects","text":"For projects that span multiple PIs, have dedicated project funding, or other special requirements which don\u2019t fit well within a PI associated lab account, a special project can be created. Projects are set up on a case by case basis around the specific needs and requirements for each one. Mail the SCG Action Team to request setting up a project.","title":"Projects"},{"location":"configuration/","text":"SGI Cluster Hardware Resources \u00b6 Intel Xeon Haswell Nodes \u00b6 2U Dell PowerEdge R730xd Dual Intel Xeon E5-2683 processors, 2.00GHz, 14-cores/28-threads 396GB DDR4-2133 Registered EEC memory 20 terabytes of flash memory 4 NVidia Pascal GPUs (P100s, especially suited to deep learning) 150+ terabytes of local scratch storage. The UV300 is available on the SCG cluster in the nih_s10 partition. For more information about how to use it see: Working with Slurm Using GPUs This supercomputer is made available to the Stanford community via the Genetics Bioinformatics Service Center (GBSC). For more background information about the UV-300 and the GBSC, see the GBSC website . Configuration Details \u00b6 High memory-to-processor ratio Intel Xeon E7-8867 v4 with 24 CPUs/socket SGI NUMALink\u2122 7 interconnect (NL7; 7.47GB/s bidirectional peak) Ultra low latency: All-to-All & Multi-dimensional All-to-all network topology Extreme I/O: 12 PCIe Gen3 slots per chassis NVidia Pascal GPUs 5.3 TFLOPS of double-precision floating point (FP64) performance 10.6 TFLOPS of single-precision floating point (FP32) performance 21.2 TFLOPS of half-precision floating point (FP16) performance NVIDIA\u2019s NVLink: provides GPU-to-GPU data transfers at up to 160 Gigabytes/second of bidirectional bandwidth HBM2: offering three times (3x) the memory bandwidth of the Maxwell GM200 GPU Unified Memory and Compute Preemption via CUDA 8+","title":"SGI Cluster Hardware Resources"},{"location":"configuration/#sgi-cluster-hardware-resources","text":"","title":"SGI Cluster Hardware Resources"},{"location":"configuration/#intel-xeon-haswell-nodes","text":"2U Dell PowerEdge R730xd Dual Intel Xeon E5-2683 processors, 2.00GHz, 14-cores/28-threads 396GB DDR4-2133 Registered EEC memory 20 terabytes of flash memory 4 NVidia Pascal GPUs (P100s, especially suited to deep learning) 150+ terabytes of local scratch storage. The UV300 is available on the SCG cluster in the nih_s10 partition. For more information about how to use it see: Working with Slurm Using GPUs This supercomputer is made available to the Stanford community via the Genetics Bioinformatics Service Center (GBSC). For more background information about the UV-300 and the GBSC, see the GBSC website .","title":"Intel Xeon Haswell Nodes"},{"location":"configuration/#configuration-details","text":"High memory-to-processor ratio Intel Xeon E7-8867 v4 with 24 CPUs/socket SGI NUMALink\u2122 7 interconnect (NL7; 7.47GB/s bidirectional peak) Ultra low latency: All-to-All & Multi-dimensional All-to-all network topology Extreme I/O: 12 PCIe Gen3 slots per chassis NVidia Pascal GPUs 5.3 TFLOPS of double-precision floating point (FP64) performance 10.6 TFLOPS of single-precision floating point (FP32) performance 21.2 TFLOPS of half-precision floating point (FP16) performance NVIDIA\u2019s NVLink: provides GPU-to-GPU data transfers at up to 160 Gigabytes/second of bidirectional bandwidth HBM2: offering three times (3x) the memory bandwidth of the Maxwell GM200 GPU Unified Memory and Compute Preemption via CUDA 8+","title":"Configuration Details"},{"location":"contact/","text":"SCG Slack Workspace \u00b6 The primary means of communication around the SCG cluster is the SCG Slack Workspace . Anyone can join the workspace using their Stanford SUNet login. Once in the workspace see: Channel/User Description #general A space for people to ask general questions about SCG usage, applications, problems, \u2026 #announce Place where SCG related announcements are made. Outages, maintenance, configuration changes, seminars, related services, \u2026 @griznog SCG System Administrator. Direct message to report problems, ask questions, request software, \u2026 SCG Announce Mailing List \u00b6 The SCG Announce mailing list is a low traffic list intended for major outage notice, office hour session and other SCG related announcements that are of interest to the entire SCG community. You can subscribe here . Note that the #announce channel in the SCG Slack Workspace will have these announcements plus additional details that impact subsets of SCG users and is the more complete and up-to-date location to get SCG announcements and updates. SCG Action Mailing List \u00b6 Questions, probelms and requests can be sent to the SCG Action Mailing List . This list is seen by the SCG development teams and the sysadmins and messages sent here will automatically create a ticket with the Stanford Research Computing Center . Stanford Research Computing Center \u00b6 If all else fails, email directly to Stanford Research Computing Center support at research-computing-support@stanford.edu . SCG Office Hours \u00b6 SCG also holds bi-weekly office hours open for walk-in questions. These are announced on the SCG Announce mailing list .","title":"Getting Help"},{"location":"contact/#scg-slack-workspace","text":"The primary means of communication around the SCG cluster is the SCG Slack Workspace . Anyone can join the workspace using their Stanford SUNet login. Once in the workspace see: Channel/User Description #general A space for people to ask general questions about SCG usage, applications, problems, \u2026 #announce Place where SCG related announcements are made. Outages, maintenance, configuration changes, seminars, related services, \u2026 @griznog SCG System Administrator. Direct message to report problems, ask questions, request software, \u2026","title":"SCG Slack Workspace"},{"location":"contact/#scg-announce-mailing-list","text":"The SCG Announce mailing list is a low traffic list intended for major outage notice, office hour session and other SCG related announcements that are of interest to the entire SCG community. You can subscribe here . Note that the #announce channel in the SCG Slack Workspace will have these announcements plus additional details that impact subsets of SCG users and is the more complete and up-to-date location to get SCG announcements and updates.","title":"SCG Announce Mailing List"},{"location":"contact/#scg-action-mailing-list","text":"Questions, probelms and requests can be sent to the SCG Action Mailing List . This list is seen by the SCG development teams and the sysadmins and messages sent here will automatically create a ticket with the Stanford Research Computing Center .","title":"SCG Action Mailing List"},{"location":"contact/#stanford-research-computing-center","text":"If all else fails, email directly to Stanford Research Computing Center support at research-computing-support@stanford.edu .","title":"Stanford Research Computing Center"},{"location":"contact/#scg-office-hours","text":"SCG also holds bi-weekly office hours open for walk-in questions. These are announced on the SCG Announce mailing list .","title":"SCG Office Hours"},{"location":"covid19/","text":"Special Access for COVID-19 Research \u00b6 If your work is related to the COVID-19 virus, you can apply for Enhanced Access to the SCG Cluster. With this access, we will give your compute jobs on the SCG Cluster a higher priority, which means they will execute sooner, wait less, and run more often. This Enhanced Access will NOT prevent your jobs from waiting. It will only ensure that they will be given priority in scheduling when resources are available. Enhanced Access COVID-19 jobs will be considered for scheduling above all other pending jobs, within the limits described at the bottom of this page. Requesting Access \u00b6 To get Enhanced Access; please fill out the application form with your name, SUNetID, and SUNetID of your lab\u2019s PI; and give us a short paragraph about how your work relates to COVID-19 research and how better access to the cluster can help you achieve your goals faster and more efficiently. This application will be sent to your PI for their approval. If you would like to add another lab member on to an existing authorization, you do not need to fill in the survey again. Instead, email scg-action@lists.stanford.edu , CCing your PI. In the email, provide your SUNetID, the SUNetID of your lab\u2019s PI, and the SUNetID of the person who should be added to the existing request. Your PI will need to approve the change (a simple reply-all to your email is fine). How to Use \u00b6 Priority is provided for batch jobs using the batch and nih_s10 partitions. Your job should be able to run using the sbatch command, so you should start by writing your batch script as normal. High-priority access is being implemented using a SLURM QoS. Once your batch script is ready, you should modify it to use the new covid19 QoS. You can do this one of two ways: In your sbatch command line, add the option -q covid19 (or --qos=covid19 ). In the batch script, near the top, add the line #SBATCH --qos=covid19 . If you use this method, the line must appear before any commands. The first method applies the QoS to only that specific job submission. The second method applies the QoS to all job submissions using that batch script. REMEMBER: This QoS may only be used for jobs related to COVID-19 research. If you have a pipeline that is being used for COVID-19 and non-COVID-19 research, you should use method 1 (the -q covid19 method) only for the COVID-19 runs. Once your job has been submitted, to confirm that your job is using the covid19 QOS, you can run this command: scontrol show job JOBID (Replace JOBID with your job\u2019s ID number.) Look for the QOS entry (near the top of the output) to confirm that you are using the covid19 QoS. If you already have a job submitted, and it is not yet running, you can use the following command to change the job to use the new covid19 QoS: scontrol update jobid=JOBID9 QOS=covid19 (Again, substitute your job\u2019s ID number.) Note that the usual SLURM lag times still apply: Once a job is submitted (or modified), it can take a minute or two before it is run. When the cluster is being fully-utilized, you job will still wait. If your job is waiting because of (Resources) , your job is waiting for other jobs to finish. COVID-19 jobs will not preempt/terminate non-COVID-19 jobs. As soon as enough jobs complete to free up the necessary resources, your job will run. If your job is waiting for any (QOS...) reason, that means one or more of the below limits have been reached. As other COVID-19 jobs complete, your job will begin. Current Limits \u00b6 At this time (June 9 @ 15:00 US/Pacific), the following limits are in place: Each user may not have more than 5 jobs running under this QoS. Each group may not have more than 15 jobs running under this QoS. When adding up the number of CPU cores in use, no more than 180 may be in use by this QoS. We will be monitoring the cluster and reserve the right to adjust limits (up or down). If we adjust limits down, we will not cancel any currently-running jobs.","title":"COVID-19 Access"},{"location":"covid19/#special-access-for-covid-19-research","text":"If your work is related to the COVID-19 virus, you can apply for Enhanced Access to the SCG Cluster. With this access, we will give your compute jobs on the SCG Cluster a higher priority, which means they will execute sooner, wait less, and run more often. This Enhanced Access will NOT prevent your jobs from waiting. It will only ensure that they will be given priority in scheduling when resources are available. Enhanced Access COVID-19 jobs will be considered for scheduling above all other pending jobs, within the limits described at the bottom of this page.","title":"Special Access for COVID-19 Research"},{"location":"covid19/#requesting-access","text":"To get Enhanced Access; please fill out the application form with your name, SUNetID, and SUNetID of your lab\u2019s PI; and give us a short paragraph about how your work relates to COVID-19 research and how better access to the cluster can help you achieve your goals faster and more efficiently. This application will be sent to your PI for their approval. If you would like to add another lab member on to an existing authorization, you do not need to fill in the survey again. Instead, email scg-action@lists.stanford.edu , CCing your PI. In the email, provide your SUNetID, the SUNetID of your lab\u2019s PI, and the SUNetID of the person who should be added to the existing request. Your PI will need to approve the change (a simple reply-all to your email is fine).","title":"Requesting Access"},{"location":"covid19/#how-to-use","text":"Priority is provided for batch jobs using the batch and nih_s10 partitions. Your job should be able to run using the sbatch command, so you should start by writing your batch script as normal. High-priority access is being implemented using a SLURM QoS. Once your batch script is ready, you should modify it to use the new covid19 QoS. You can do this one of two ways: In your sbatch command line, add the option -q covid19 (or --qos=covid19 ). In the batch script, near the top, add the line #SBATCH --qos=covid19 . If you use this method, the line must appear before any commands. The first method applies the QoS to only that specific job submission. The second method applies the QoS to all job submissions using that batch script. REMEMBER: This QoS may only be used for jobs related to COVID-19 research. If you have a pipeline that is being used for COVID-19 and non-COVID-19 research, you should use method 1 (the -q covid19 method) only for the COVID-19 runs. Once your job has been submitted, to confirm that your job is using the covid19 QOS, you can run this command: scontrol show job JOBID (Replace JOBID with your job\u2019s ID number.) Look for the QOS entry (near the top of the output) to confirm that you are using the covid19 QoS. If you already have a job submitted, and it is not yet running, you can use the following command to change the job to use the new covid19 QoS: scontrol update jobid=JOBID9 QOS=covid19 (Again, substitute your job\u2019s ID number.) Note that the usual SLURM lag times still apply: Once a job is submitted (or modified), it can take a minute or two before it is run. When the cluster is being fully-utilized, you job will still wait. If your job is waiting because of (Resources) , your job is waiting for other jobs to finish. COVID-19 jobs will not preempt/terminate non-COVID-19 jobs. As soon as enough jobs complete to free up the necessary resources, your job will run. If your job is waiting for any (QOS...) reason, that means one or more of the below limits have been reached. As other COVID-19 jobs complete, your job will begin.","title":"How to Use"},{"location":"covid19/#current-limits","text":"At this time (June 9 @ 15:00 US/Pacific), the following limits are in place: Each user may not have more than 5 jobs running under this QoS. Each group may not have more than 15 jobs running under this QoS. When adding up the number of CPU cores in use, no more than 180 may be in use by this QoS. We will be monitoring the cluster and reserve the right to adjust limits (up or down). If we adjust limits down, we will not cancel any currently-running jobs.","title":"Current Limits"},{"location":"quick_start/","text":"Accessing SCG \u00b6 SCG OnDemand \u00b6 SCG OnDemand is a web-based interface to SCG cluster resources. OnDemand offers terminal, file manager and editor and GUI access to an SCG desktop session and GUI applications right from a web browser. SSH (Terminal) \u00b6 From the terminal application of your choice, ssh to login.scg.stanford.edu with your SUNetID username and password, ssh SUNetID@login.scg.stanford.edu Data Management \u00b6 Data can be moved in and out of SCG with a number of tools/methods. SCG OnDemand File App Globus (endpoint: SCG Cluster Storage) Samba rsync scp/sftp For more information see Managing and Moving Data . Software \u00b6 Environment Modules \u00b6 SCG uses modules to manage software not installed into the base operating system image. Basic usage is shown below: Command Description module avail List available software module load APP/VERSION Load APP/VERSION into working environment module unload APP/VERSION Remove APP/VERSION from the working environment module purge Remove all loaded modules from working environment Slurm Job Submission \u00b6 The SCG Cluster uses Slurm as it\u2019s job scheduler, standard Slurm job scripts and commands will work, noting these SCG specific requirements: SCG partitions are batch , interactive and nih_s10 . The batch and nih_s10 partitions require specifying --account=PI_SUNetID where PI_SUNetID is the SUNetID of the PI for the job accounting. A --time= time limit must be specified. SCG is limited to single node jobs, so most jobs will want --nodes=1 --ntasks=1 --cpus-per-task=N where N is the number of cores for a multithreaded application.","title":"Quick Start"},{"location":"quick_start/#accessing-scg","text":"","title":"Accessing SCG"},{"location":"quick_start/#scg-ondemand","text":"SCG OnDemand is a web-based interface to SCG cluster resources. OnDemand offers terminal, file manager and editor and GUI access to an SCG desktop session and GUI applications right from a web browser.","title":"SCG OnDemand"},{"location":"quick_start/#ssh-terminal","text":"From the terminal application of your choice, ssh to login.scg.stanford.edu with your SUNetID username and password, ssh SUNetID@login.scg.stanford.edu","title":"SSH (Terminal)"},{"location":"quick_start/#data-management","text":"Data can be moved in and out of SCG with a number of tools/methods. SCG OnDemand File App Globus (endpoint: SCG Cluster Storage) Samba rsync scp/sftp For more information see Managing and Moving Data .","title":"Data Management"},{"location":"quick_start/#software","text":"","title":"Software"},{"location":"quick_start/#environment-modules","text":"SCG uses modules to manage software not installed into the base operating system image. Basic usage is shown below: Command Description module avail List available software module load APP/VERSION Load APP/VERSION into working environment module unload APP/VERSION Remove APP/VERSION from the working environment module purge Remove all loaded modules from working environment","title":"Environment Modules"},{"location":"quick_start/#slurm-job-submission","text":"The SCG Cluster uses Slurm as it\u2019s job scheduler, standard Slurm job scripts and commands will work, noting these SCG specific requirements: SCG partitions are batch , interactive and nih_s10 . The batch and nih_s10 partitions require specifying --account=PI_SUNetID where PI_SUNetID is the SUNetID of the PI for the job accounting. A --time= time limit must be specified. SCG is limited to single node jobs, so most jobs will want --nodes=1 --ntasks=1 --cpus-per-task=N where N is the number of cores for a multithreaded application.","title":"Slurm Job Submission"},{"location":"software/","text":"Software \u00b6 This will serve as a helpful guide to interacting with SCG software. We will add libraries on demand. Please open an issue to request information added on packages. Sequence Read Archive \u00b6 To download data from the Sequence Read Archive you can use the sra-toolkit that is available to you on SCG. Here is how to download the file linked above: # load the sratoolkit module ml sratoolkit # get the zipped fast1 for the run fastq-dump --split-files --gzip SRR2045608 Want to see other software packages added here? let us know","title":"Software"},{"location":"software/#software","text":"This will serve as a helpful guide to interacting with SCG software. We will add libraries on demand. Please open an issue to request information added on packages.","title":"Software"},{"location":"software/#sequence-read-archive","text":"To download data from the Sequence Read Archive you can use the sra-toolkit that is available to you on SCG. Here is how to download the file linked above: # load the sratoolkit module ml sratoolkit # get the zipped fast1 for the run fastq-dump --split-files --gzip SRR2045608 Want to see other software packages added here? let us know","title":"Sequence Read Archive"},{"location":"uv300/","text":"SGI UV300 SuperComputer \u00b6 Stanford provides access to an on-site supercomputer for biomedical researchers who need to run analyses requiring large, dedicated computing resources to be applied to their solutions simultaneously, such as many CPUs, large memory, fast local storage, and/or high-powered GPUs. This supercomputer, an SGI (now part of Hewlett Packard Enterprise) UV300 machine, was acquired via a NIH S10 Shared Instrumentation Grant. It has: 360 cores 10 terabytes of random-access memory (RAM) 20 terabytes of flash memory 4 NVidia Pascal GPUs (P100s, especially suited to deep learning) 150+ terabytes of local scratch storage. The UV300 is available on the SCG cluster in the nih_s10 partition. For more information about how to use it see: Working with Slurm Using GPUs This supercomputer is made available to the Stanford community via the Genetics Bioinformatics Service Center (GBSC). For more background information about the UV-300 and the GBSC, see the GBSC website . Configuration Details \u00b6 High memory-to-processor ratio Intel Xeon E7-8867 v4 with 24 CPUs/socket SGI NUMALink\u2122 7 interconnect (NL7; 7.47GB/s bidirectional peak) Ultra low latency: All-to-All & Multi-dimensional All-to-all network topology Extreme I/O: 12 PCIe Gen3 slots per chassis NVidia Pascal GPUs 5.3 TFLOPS of double-precision floating point (FP64) performance 10.6 TFLOPS of single-precision floating point (FP32) performance 21.2 TFLOPS of half-precision floating point (FP16) performance NVIDIA\u2019s NVLink: provides GPU-to-GPU data transfers at up to 160 Gigabytes/second of bidirectional bandwidth HBM2: offering three times (3x) the memory bandwidth of the Maxwell GM200 GPU Unified Memory and Compute Preemption via CUDA 8+","title":"NIH Supercomputer (UV300)"},{"location":"uv300/#sgi-uv300-supercomputer","text":"Stanford provides access to an on-site supercomputer for biomedical researchers who need to run analyses requiring large, dedicated computing resources to be applied to their solutions simultaneously, such as many CPUs, large memory, fast local storage, and/or high-powered GPUs. This supercomputer, an SGI (now part of Hewlett Packard Enterprise) UV300 machine, was acquired via a NIH S10 Shared Instrumentation Grant. It has: 360 cores 10 terabytes of random-access memory (RAM) 20 terabytes of flash memory 4 NVidia Pascal GPUs (P100s, especially suited to deep learning) 150+ terabytes of local scratch storage. The UV300 is available on the SCG cluster in the nih_s10 partition. For more information about how to use it see: Working with Slurm Using GPUs This supercomputer is made available to the Stanford community via the Genetics Bioinformatics Service Center (GBSC). For more background information about the UV-300 and the GBSC, see the GBSC website .","title":"SGI UV300 SuperComputer"},{"location":"uv300/#configuration-details","text":"High memory-to-processor ratio Intel Xeon E7-8867 v4 with 24 CPUs/socket SGI NUMALink\u2122 7 interconnect (NL7; 7.47GB/s bidirectional peak) Ultra low latency: All-to-All & Multi-dimensional All-to-all network topology Extreme I/O: 12 PCIe Gen3 slots per chassis NVidia Pascal GPUs 5.3 TFLOPS of double-precision floating point (FP64) performance 10.6 TFLOPS of single-precision floating point (FP32) performance 21.2 TFLOPS of half-precision floating point (FP16) performance NVIDIA\u2019s NVLink: provides GPU-to-GPU data transfers at up to 160 Gigabytes/second of bidirectional bandwidth HBM2: offering three times (3x) the memory bandwidth of the Maxwell GM200 GPU Unified Memory and Compute Preemption via CUDA 8+","title":"Configuration Details"},{"location":"faqs/account/","text":"Is --account=... required? \u00b6 All jobs ran on SCG get charged to an account, in some cases for real money and in others just for tracking and thus require an --account= specification. There are a number of ways to set this for jobs: sbatch/srun/salloc command line option: --account=default|PISUNetID|ProjectID In the job script: #SBATCH --account=PISUNetID|ProjectID Setting a default in your $HOME/.bashrc: export SBATCH_ACCOUNT=default|PISUNetID|ProjectID Note: SBATCH_ACCOUNT Will be overridden by job script and command line values, if they are used. For all partitions \u2018\u2019\u2018except\u2019\u2018\u2019 interactive, use the PISUNetID or ProjectID to which the job should be charged. For most people the only available option is a single PISUNetID for the lab/group they are a member of, but those who work across several lab or projects will need to keep closer track of the account to be charged. For the interactive partition there is no charge for usage and so the default account should be used. I already submitted a bunch of jobs but put in the wrong account, how can I fix this? \u00b6 Not a problem, simply run for each pending JOBID : scontrol update jobid=JOBID account=default|PISUNetID|ProjectID If your job has already entered the running state you will need to kill it ( scancel JOBID ) and resubmit to change the account. How do I know what accounts I can submit with? \u00b6 A utility has been provided to list the details of your SCG cluster account, try scgwhoami , for example: [ griznog@smsx10srw-srcf-d15-37 ~ ] $ scgwhoami SCG Account Information Real Name : John Hanks username : griznog uidNumber : 325892 $ HOME : / home / griznog primary group : upg_griznog gidNumber : 3772 secondary groups : scg - admin , scg - users , scgpm - informatics_wu , scg_cluster_users , scg_lab_joewu , scg_cluster_admins , scg_cluster_apps , scg_prj_clinical_service , scg_prj_gbsc Available SLURM Accounts clinical_service gbsc default","title":"--account=?"},{"location":"faqs/account/#is-account-required","text":"All jobs ran on SCG get charged to an account, in some cases for real money and in others just for tracking and thus require an --account= specification. There are a number of ways to set this for jobs: sbatch/srun/salloc command line option: --account=default|PISUNetID|ProjectID In the job script: #SBATCH --account=PISUNetID|ProjectID Setting a default in your $HOME/.bashrc: export SBATCH_ACCOUNT=default|PISUNetID|ProjectID Note: SBATCH_ACCOUNT Will be overridden by job script and command line values, if they are used. For all partitions \u2018\u2019\u2018except\u2019\u2018\u2019 interactive, use the PISUNetID or ProjectID to which the job should be charged. For most people the only available option is a single PISUNetID for the lab/group they are a member of, but those who work across several lab or projects will need to keep closer track of the account to be charged. For the interactive partition there is no charge for usage and so the default account should be used.","title":"Is --account=... required?"},{"location":"faqs/account/#i-already-submitted-a-bunch-of-jobs-but-put-in-the-wrong-account-how-can-i-fix-this","text":"Not a problem, simply run for each pending JOBID : scontrol update jobid=JOBID account=default|PISUNetID|ProjectID If your job has already entered the running state you will need to kill it ( scancel JOBID ) and resubmit to change the account.","title":"I already submitted a bunch of jobs but put in the wrong account, how can I fix this?"},{"location":"faqs/account/#how-do-i-know-what-accounts-i-can-submit-with","text":"A utility has been provided to list the details of your SCG cluster account, try scgwhoami , for example: [ griznog@smsx10srw-srcf-d15-37 ~ ] $ scgwhoami SCG Account Information Real Name : John Hanks username : griznog uidNumber : 325892 $ HOME : / home / griznog primary group : upg_griznog gidNumber : 3772 secondary groups : scg - admin , scg - users , scgpm - informatics_wu , scg_cluster_users , scg_lab_joewu , scg_cluster_admins , scg_cluster_apps , scg_prj_clinical_service , scg_prj_gbsc Available SLURM Accounts clinical_service gbsc default","title":"How do I know what accounts I can submit with?"},{"location":"faqs/cores/","text":"nodes vs tasks vs cpus vs cores \u00b6 A combination of raw technical detail, Slurm\u2019s loose usage of the terms core and cpu and multiple models of parallel computing require establishing a bit of background to fully explain how to make efficient use of multiple cores on SCG. Before we delve into that\u2026 TL;DR \u00b6 For the impatient reader, in 99.9% of the cases SCG was designed to support, #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=N is the correct way to request N cores for a job. Just replace N in that config with the number of cores you need and optionally inside job scripts use the ${SLURM_CPUS_PER_TASK} variable to pass the number of cores in the job to commands that accept an argument for number of core, cpus or threads, e.g., mycommand -arg1 -arg2 --threads= ${ SLURM_CPUS_PER_TASK } ... Computer architecture \u00b6 The parts of a modern computer we need to understand to apply to running jobs are listed here. (Note: This is way oversimplified and intended to give a basic overview for the purposes of understanding how to request resources from Slurm, there are a lot of resources out there to dig deeper into computer architecture.) Board A physical motherboard which contains one or more of each of Socket, Memory bus and PCI bus. Socket A physical socket on a motherboard which accepts a physical CPU part. CPU A physical part that is plugged into a socket. Core A physical CPU core, one of many possible cores, that are part of a CPU. HyperThread A virtual CPU thread, associated with a specific Core. This can be enabled or disabled on a system. SCG typically disabled hyperthreading. Memory Bus A communication bus between system memory and a Socket/CPU. PCI Bus A communication bus between a Socket/CPU and I/O controllers (disks, networking, graphics,...) in the server. Slurm complicates this, however, by using the terms core and cpu interchangeably depending on the context and Slurm command. --cpus-per-taks= for example is actually specifying the number of cores per task. Parallel Computing \u00b6 A very simplified description of parallel computing which is sufficient for most computing is to break parallel workloads into these three categories: Fine grained Each process does a lot of communication with the other processes. Think of a model which uses a grid of cells, and at each iteration all the cells exchange information with their neighboring cells. These types of applications are typically limited by interprocess communication bandwidth. Coarse grained Each process occasionally comunicates with other processes. A model which has independent units that occasionally exchange information globally would fall into this category. These types of processes are typically CPU or I/O limited. Embarrassingly Parallel Processes do not communicate with each other and run completely independently, possibly including a step to merge results when the last process finishes. These types of processes are typically CPU or I/O limited. Many bioinformatics tasks fall into the last category. Alignment, for instance, can be easily parallelized by breaking input sequences into subsets which are then individualy aligned to a reference and the results merged/sorted after all subsets are complete. Some tools do this internally, e.g. running BLAST with some number of threads allows BLAST to place subsets of input sequence into each thread and then align against a shared in-memory reference, thus giving a pretty good speedup to the overall alignment task. As the above example implies, when a task is broken up to be parallelized, the subtasks can be ran in more than one way. Models we care about for this are: MPI \u00b6 MPI (Message Passing Interface) is not very common in bioinformatics tools as it is (in the opinion of this sysadmin) difficult to debug and use correctly and is overkill for breaking up embarassingly parallel tasks. It\u2019s so rare, in fact, that SCG by default rejects jobs that request more than one node on the assumption that doing so is almost always an error in the job submission. A special hidden partition is available for running MPI jobs upon request. MPI is really a special case of using Tasks (see below) to parallelze work but it has a complicated enough wrapper to deserve its own category. Threads \u00b6 Most bioinformatics tools that include a parallel option in the application use threading, with the most commonly used implementation being OpenMP. Applications with take an argument specifying cores, threads or CPUs are very likely to be using this model to parallelize the work. Threads can make efficient use of memory for things like holding reference data used by multiple threads/processes while running. Tasks \u00b6 The most common example of parallelizing this way on SCG is to take a directory with many different sequence files and use a job script or job array to run the same command on all files, placing each command into its own job. This is what most bioinformatics workloads look like, at some point during a project when there are too many tasks or the tasks are too large to handle on a laptop or workstation, they can easily be broken into jobs to run in parallel on the SCG cluster resources.","title":"CPUs/Nodes/Tasks?"},{"location":"faqs/cores/#nodes-vs-tasks-vs-cpus-vs-cores","text":"A combination of raw technical detail, Slurm\u2019s loose usage of the terms core and cpu and multiple models of parallel computing require establishing a bit of background to fully explain how to make efficient use of multiple cores on SCG. Before we delve into that\u2026","title":"nodes vs tasks vs cpus vs cores"},{"location":"faqs/cores/#tldr","text":"For the impatient reader, in 99.9% of the cases SCG was designed to support, #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=N is the correct way to request N cores for a job. Just replace N in that config with the number of cores you need and optionally inside job scripts use the ${SLURM_CPUS_PER_TASK} variable to pass the number of cores in the job to commands that accept an argument for number of core, cpus or threads, e.g., mycommand -arg1 -arg2 --threads= ${ SLURM_CPUS_PER_TASK } ...","title":"TL;DR"},{"location":"faqs/cores/#computer-architecture","text":"The parts of a modern computer we need to understand to apply to running jobs are listed here. (Note: This is way oversimplified and intended to give a basic overview for the purposes of understanding how to request resources from Slurm, there are a lot of resources out there to dig deeper into computer architecture.) Board A physical motherboard which contains one or more of each of Socket, Memory bus and PCI bus. Socket A physical socket on a motherboard which accepts a physical CPU part. CPU A physical part that is plugged into a socket. Core A physical CPU core, one of many possible cores, that are part of a CPU. HyperThread A virtual CPU thread, associated with a specific Core. This can be enabled or disabled on a system. SCG typically disabled hyperthreading. Memory Bus A communication bus between system memory and a Socket/CPU. PCI Bus A communication bus between a Socket/CPU and I/O controllers (disks, networking, graphics,...) in the server. Slurm complicates this, however, by using the terms core and cpu interchangeably depending on the context and Slurm command. --cpus-per-taks= for example is actually specifying the number of cores per task.","title":"Computer architecture"},{"location":"faqs/cores/#parallel-computing","text":"A very simplified description of parallel computing which is sufficient for most computing is to break parallel workloads into these three categories: Fine grained Each process does a lot of communication with the other processes. Think of a model which uses a grid of cells, and at each iteration all the cells exchange information with their neighboring cells. These types of applications are typically limited by interprocess communication bandwidth. Coarse grained Each process occasionally comunicates with other processes. A model which has independent units that occasionally exchange information globally would fall into this category. These types of processes are typically CPU or I/O limited. Embarrassingly Parallel Processes do not communicate with each other and run completely independently, possibly including a step to merge results when the last process finishes. These types of processes are typically CPU or I/O limited. Many bioinformatics tasks fall into the last category. Alignment, for instance, can be easily parallelized by breaking input sequences into subsets which are then individualy aligned to a reference and the results merged/sorted after all subsets are complete. Some tools do this internally, e.g. running BLAST with some number of threads allows BLAST to place subsets of input sequence into each thread and then align against a shared in-memory reference, thus giving a pretty good speedup to the overall alignment task. As the above example implies, when a task is broken up to be parallelized, the subtasks can be ran in more than one way. Models we care about for this are:","title":"Parallel Computing"},{"location":"faqs/cores/#mpi","text":"MPI (Message Passing Interface) is not very common in bioinformatics tools as it is (in the opinion of this sysadmin) difficult to debug and use correctly and is overkill for breaking up embarassingly parallel tasks. It\u2019s so rare, in fact, that SCG by default rejects jobs that request more than one node on the assumption that doing so is almost always an error in the job submission. A special hidden partition is available for running MPI jobs upon request. MPI is really a special case of using Tasks (see below) to parallelze work but it has a complicated enough wrapper to deserve its own category.","title":"MPI"},{"location":"faqs/cores/#threads","text":"Most bioinformatics tools that include a parallel option in the application use threading, with the most commonly used implementation being OpenMP. Applications with take an argument specifying cores, threads or CPUs are very likely to be using this model to parallelize the work. Threads can make efficient use of memory for things like holding reference data used by multiple threads/processes while running.","title":"Threads"},{"location":"faqs/cores/#tasks","text":"The most common example of parallelizing this way on SCG is to take a directory with many different sequence files and use a job script or job array to run the same command on all files, placing each command into its own job. This is what most bioinformatics workloads look like, at some point during a project when there are too many tasks or the tasks are too large to handle on a laptop or workstation, they can easily be broken into jobs to run in parallel on the SCG cluster resources.","title":"Tasks"},{"location":"faqs/memory/","text":"Thanks, for the memories\u2026 \u00b6 In more traditional HPC it\u2019s usually possible to predict how much memory an application will need for a given set of input data and parameters. However, in bionformatics work the ability to accurately predict memory usage is rare. Guess 1: Tool provided estimate \u00b6 The first thing to do is check the tool documentation and any available forums to see if there is already a heuristic for estimating memory use and use it. This isn\u2019t always available, so if not then\u2026 Guess 2: How big is the input data? \u00b6 A safe assumption is that the application will try to place all the data (sequence, reference, \u2026) in memory. Exceptions might be tools that convert from one format to another which may, in fact stream the data and use very little memory. But alignment, searching and assembly can be assumed to, if not require enough memory to hold the entire data set, to at least run faster if that memory is avaialble. It\u2019s usually safe to assume that a tool is storing bases as characters in strings, which means you\u2019ll need at least 1 byte per base. This makes the math easy. For example with 10 Gbases of data, 10 Gbases == 10 GB For safety sake, add an extra 4 GB to the data size based estimate and give it a try. Guess 3: What is the application? \u00b6 If the data size based guess proves to be inadequate, then next consider what the tools is doing. File Format conversion: Try doubling the estimate, maybe the data is buffered in and out so 2x data size is the next step. Alignment: A tool that uses multiple threads may need additional memory, try at least doubling the memory and possibly giving the app (Gbases * #cores) + 4GB . Assembly: Assemblers can be all over the place. If the first estimate failed, then quadrupling memory is a good start. If the assembler uses a graph based algorithm then memory usage can really skyrocket. Guess 4: Feed me, Seymore\u2026 \u00b6 If all else fails, double memory until it works. If the job is taking a really long time to reach the out-of-memory failure, then consider running on the UV300 and just giving it a monstrous amount of memory. If you have many similar jobs to run, then doing this first and using srun in your job script to start the task will prodcue some meomry use statistics in the job record which can then be used to better tune subsequent runs. Howver, note again that assemblers in particular can be all over the place on memory usage with even minor changes in sensitivity and data so some tasks are just inherently unpredictable and throwing buckets of memory at them is the most expeident method.","title":"Memory?"},{"location":"faqs/memory/#thanks-for-the-memories","text":"In more traditional HPC it\u2019s usually possible to predict how much memory an application will need for a given set of input data and parameters. However, in bionformatics work the ability to accurately predict memory usage is rare.","title":"Thanks, for the memories..."},{"location":"faqs/memory/#guess-1-tool-provided-estimate","text":"The first thing to do is check the tool documentation and any available forums to see if there is already a heuristic for estimating memory use and use it. This isn\u2019t always available, so if not then\u2026","title":"Guess 1: Tool provided estimate"},{"location":"faqs/memory/#guess-2-how-big-is-the-input-data","text":"A safe assumption is that the application will try to place all the data (sequence, reference, \u2026) in memory. Exceptions might be tools that convert from one format to another which may, in fact stream the data and use very little memory. But alignment, searching and assembly can be assumed to, if not require enough memory to hold the entire data set, to at least run faster if that memory is avaialble. It\u2019s usually safe to assume that a tool is storing bases as characters in strings, which means you\u2019ll need at least 1 byte per base. This makes the math easy. For example with 10 Gbases of data, 10 Gbases == 10 GB For safety sake, add an extra 4 GB to the data size based estimate and give it a try.","title":"Guess 2: How big is the input data?"},{"location":"faqs/memory/#guess-3-what-is-the-application","text":"If the data size based guess proves to be inadequate, then next consider what the tools is doing. File Format conversion: Try doubling the estimate, maybe the data is buffered in and out so 2x data size is the next step. Alignment: A tool that uses multiple threads may need additional memory, try at least doubling the memory and possibly giving the app (Gbases * #cores) + 4GB . Assembly: Assemblers can be all over the place. If the first estimate failed, then quadrupling memory is a good start. If the assembler uses a graph based algorithm then memory usage can really skyrocket.","title":"Guess 3: What is the application?"},{"location":"faqs/memory/#guess-4-feed-me-seymore","text":"If all else fails, double memory until it works. If the job is taking a really long time to reach the out-of-memory failure, then consider running on the UV300 and just giving it a monstrous amount of memory. If you have many similar jobs to run, then doing this first and using srun in your job script to start the task will prodcue some meomry use statistics in the job record which can then be used to better tune subsequent runs. Howver, note again that assemblers in particular can be all over the place on memory usage with even minor changes in sensitivity and data so some tasks are just inherently unpredictable and throwing buckets of memory at them is the most expeident method.","title":"Guess 4: Feed me, Seymore..."},{"location":"faqs/oak_migration/","text":"SCG Storage migration from Isilon (OneFS) to Oak (Lustre) \u00b6 A number of details about SCG storage change with the migration from the Isilon OneFS storage to the Oak Lustre storage. The key points are: All ACLs in use on SCG are now POSIX ACLs. ACLs can be set/managed with the setfacl and getfacl commands. df now returns the space for the entire Oak filesystem rather than information for a lab or project specific quota. Oak quotas for a given PISUNetID can be viewed with: lfs quota -h -g scg_lab_PISUNetID /oak The old Isilon OneFS storage is mounted read-only under /deprecated/ifs but is subject to removal with NO ADVANCE NOTICE . Every effort was made to ensure data was completely copied to Oak, but this will be temporarily available for comparing and retrieving any NFS4 ACLs from the old filesystem. With the move to Oak, inodes now matter. It\u2019s more important now to bundle large numbers of small files using tar or zip or similar utilities. In addition to quotas for space, labs are now subject to quotas for total number of files and either quota can be exhausted which will prevent writes to the storage unless/until quota is increased or other files/data are removed.","title":"Oak Storage Migration"},{"location":"faqs/oak_migration/#scg-storage-migration-from-isilon-onefs-to-oak-lustre","text":"A number of details about SCG storage change with the migration from the Isilon OneFS storage to the Oak Lustre storage. The key points are: All ACLs in use on SCG are now POSIX ACLs. ACLs can be set/managed with the setfacl and getfacl commands. df now returns the space for the entire Oak filesystem rather than information for a lab or project specific quota. Oak quotas for a given PISUNetID can be viewed with: lfs quota -h -g scg_lab_PISUNetID /oak The old Isilon OneFS storage is mounted read-only under /deprecated/ifs but is subject to removal with NO ADVANCE NOTICE . Every effort was made to ensure data was completely copied to Oak, but this will be temporarily available for comparing and retrieving any NFS4 ACLs from the old filesystem. With the move to Oak, inodes now matter. It\u2019s more important now to bundle large numbers of small files using tar or zip or similar utilities. In addition to quotas for space, labs are now subject to quotas for total number of files and either quota can be exhausted which will prevent writes to the storage unless/until quota is increased or other files/data are removed.","title":"SCG Storage migration from Isilon (OneFS) to Oak (Lustre)"},{"location":"faqs/time/","text":"Is --time=... required? \u00b6 Yes. Slurm uses the requested --time= for a job to determine if it is eligible for backfill, that is, to see if there is an idle slot available which may be reserved for a future job, but that future job starts far enough into the future that the smaller job would be finished before then. Having an accurate \u2013time for a job gives it the best chance of running sooner than normal as backfill and helps make the overall utilization of the cluster more efficient. Operationally, there are two ways to handle this: Set a default time if none is specified. Force --time= to be included on every job. Since the sysadmins really have no idea what a good default time would be, trying to set one can result in having jobs die before they finish, thus wasting the time spent on them (in the absence of good checkpointing) or always guessing long and making backfill less efficient. Because of this, SCG requires that --time= be specified with every job. It is understandable, however, that someone who has applications or workflows which are inherently unpredictable would just want to have some default time set for them and not worry about this. If that is desired, then adding to the ~/.bashrc file this line: export SBATCH_TIMELIMIT=4-00:00:00 Would cause sbatch to always set --time=4-00:00:00 unless it is explicitly set on the sbatch command line.","title":"--time=?"},{"location":"faqs/time/#is-time-required","text":"Yes. Slurm uses the requested --time= for a job to determine if it is eligible for backfill, that is, to see if there is an idle slot available which may be reserved for a future job, but that future job starts far enough into the future that the smaller job would be finished before then. Having an accurate \u2013time for a job gives it the best chance of running sooner than normal as backfill and helps make the overall utilization of the cluster more efficient. Operationally, there are two ways to handle this: Set a default time if none is specified. Force --time= to be included on every job. Since the sysadmins really have no idea what a good default time would be, trying to set one can result in having jobs die before they finish, thus wasting the time spent on them (in the absence of good checkpointing) or always guessing long and making backfill less efficient. Because of this, SCG requires that --time= be specified with every job. It is understandable, however, that someone who has applications or workflows which are inherently unpredictable would just want to have some default time set for them and not worry about this. If that is desired, then adding to the ~/.bashrc file this line: export SBATCH_TIMELIMIT=4-00:00:00 Would cause sbatch to always set --time=4-00:00:00 unless it is explicitly set on the sbatch command line.","title":"Is --time=... required?"},{"location":"tutorials/connecting/","text":"Cluster Overview \u00b6 The SCG cluster is a standard, HPC-style shared-resource cluster environment consisting of Hardware Compute and Login nodes Storage systems Ethernet network Software Operating System (Linux: CentOS 7.x ) Job Scheduler ( Slurm ) Software Applications CLI/Terminal Access \u00b6 Primary access to the SCG resources is via a Command Line Interface , in this case interfacing to the Linux CLI accessed remotely via Secure Shell (ssh) . To access via SSH there will need to be an SSH client installed on the client workstation, desktop or laptop or a web based terminal is available via SCG OnDemand Shell Access . For locally installed clients, depending on the Operating System used on the client system there are several options: Windows MobaXterm (free and highly recommended)] PuTTY (free) bitvise (free client) and many others\u2026 Mac OSX (note that ssh tools are built in, but there are several terminal options) Built-in Terminal app iTerm2 (free and highly recommended) Linux Everything needed is included. Because \u2026 Linux. Working at the command line is not covered here, but there are many, many resources online to help get started using the Bash shell and unix command line tools. A short list of example sites is: Linux Tutorial linuxcommand.org Bash Guide for Beginners Beginner\u2019s Guide to the Bash Terminal Example SSH connection \u00b6 Using the SSH client, connect to the cluster by ssh\u2019ing to the login nodes. login.scg.stanford.edu is an alias which will resolve to the \u201cbest\u201d login node available from a pool of login nodes: [ griznog@gambusia ~ ] $ ssh griznog @login . scg . stanford . edu griznog @login . scg . stanford . edu 's password: Duo two-factor login for griznog Enter a passcode or select one of the following options: 1. Duo Push to XXX-XXX-4102 2. Phone call to XXX-XXX-4102 3. SMS passcodes to XXX-XXX-4102 Passcode or option (1-3): 1 Success. Logging you in... Last login: Fri Jan 4 17:35:46 2019 from 171.66.185.235 __ __ _ _ \\ \\ / /__| |__ ___ _ __ ___ | |_ ___ \\ \\/\\/ / -_) / _/ _ \\ ' \\ / - _ ) | _ / _ \\ \\ _ / \\ _ / \\ ___ | _ \\ __ \\ ___ / _ | _ | _ \\ ___ | \\ __ \\ ___ / ___ __ __ _ O o O o O o ( _ -</ _ / _ ` | | O o | | O o | | O o | / __ / \\ __ \\ __ , | | | O | | | | O | | | | O | | | ___ / | o O | | o O | | o O | o O o O o O Keep up - to - date on SCG by following #announce in our Slack Workspace at https : // susciclu . slack . com Problems : Send email to scg - action @lists . stanford . edu or if you prefer a more interactive method , use your Stanford email to sign up at our Slack work - space : https : // susciclu . slack . com and ask in #general or dm to @griznog Like Disclaimers ? : This system may contain research data whose use is subject to restrictions from government agencies , contractual partners , or research sponsors . Any use of this data must be authorized by the Principal Investigator . Contact scg - action @lists . stanford . edu for more information on the terms and conditions governing the use of this data . [ griznog@smsx10srw-srcf-d15-38 ~ ] $ NOTE : SCG accepts Kerberos tickets, SSH pubkey and Password authentication as the first factor for two-factor auth. If your laptop or workstation is configured with Stanford Kerberos, you may not see a password prompt or may be able to avoid it by using kinit SUNetID@stanford.edu before starting ssh. You can also set up SSH key authentication.","title":"Connecting to SCG"},{"location":"tutorials/connecting/#cluster-overview","text":"The SCG cluster is a standard, HPC-style shared-resource cluster environment consisting of Hardware Compute and Login nodes Storage systems Ethernet network Software Operating System (Linux: CentOS 7.x ) Job Scheduler ( Slurm ) Software Applications","title":"Cluster Overview"},{"location":"tutorials/connecting/#cliterminal-access","text":"Primary access to the SCG resources is via a Command Line Interface , in this case interfacing to the Linux CLI accessed remotely via Secure Shell (ssh) . To access via SSH there will need to be an SSH client installed on the client workstation, desktop or laptop or a web based terminal is available via SCG OnDemand Shell Access . For locally installed clients, depending on the Operating System used on the client system there are several options: Windows MobaXterm (free and highly recommended)] PuTTY (free) bitvise (free client) and many others\u2026 Mac OSX (note that ssh tools are built in, but there are several terminal options) Built-in Terminal app iTerm2 (free and highly recommended) Linux Everything needed is included. Because \u2026 Linux. Working at the command line is not covered here, but there are many, many resources online to help get started using the Bash shell and unix command line tools. A short list of example sites is: Linux Tutorial linuxcommand.org Bash Guide for Beginners Beginner\u2019s Guide to the Bash Terminal","title":"CLI/Terminal Access"},{"location":"tutorials/connecting/#example-ssh-connection","text":"Using the SSH client, connect to the cluster by ssh\u2019ing to the login nodes. login.scg.stanford.edu is an alias which will resolve to the \u201cbest\u201d login node available from a pool of login nodes: [ griznog@gambusia ~ ] $ ssh griznog @login . scg . stanford . edu griznog @login . scg . stanford . edu 's password: Duo two-factor login for griznog Enter a passcode or select one of the following options: 1. Duo Push to XXX-XXX-4102 2. Phone call to XXX-XXX-4102 3. SMS passcodes to XXX-XXX-4102 Passcode or option (1-3): 1 Success. Logging you in... Last login: Fri Jan 4 17:35:46 2019 from 171.66.185.235 __ __ _ _ \\ \\ / /__| |__ ___ _ __ ___ | |_ ___ \\ \\/\\/ / -_) / _/ _ \\ ' \\ / - _ ) | _ / _ \\ \\ _ / \\ _ / \\ ___ | _ \\ __ \\ ___ / _ | _ | _ \\ ___ | \\ __ \\ ___ / ___ __ __ _ O o O o O o ( _ -</ _ / _ ` | | O o | | O o | | O o | / __ / \\ __ \\ __ , | | | O | | | | O | | | | O | | | ___ / | o O | | o O | | o O | o O o O o O Keep up - to - date on SCG by following #announce in our Slack Workspace at https : // susciclu . slack . com Problems : Send email to scg - action @lists . stanford . edu or if you prefer a more interactive method , use your Stanford email to sign up at our Slack work - space : https : // susciclu . slack . com and ask in #general or dm to @griznog Like Disclaimers ? : This system may contain research data whose use is subject to restrictions from government agencies , contractual partners , or research sponsors . Any use of this data must be authorized by the Principal Investigator . Contact scg - action @lists . stanford . edu for more information on the terms and conditions governing the use of this data . [ griznog@smsx10srw-srcf-d15-38 ~ ] $ NOTE : SCG accepts Kerberos tickets, SSH pubkey and Password authentication as the first factor for two-factor auth. If your laptop or workstation is configured with Stanford Kerberos, you may not see a password prompt or may be able to avoid it by using kinit SUNetID@stanford.edu before starting ssh. You can also set up SSH key authentication.","title":"Example SSH connection"},{"location":"tutorials/data_management/","text":"Storage Locations \u00b6 Each user has access to several storage types and locations. The following table lists the storage locations, limits and the use for which each is best suited. Location Quota Usage /home/SUNetID 32 GB Config files, important scripts, private software/data /labs/PI_SUNetID 128 GB (Free Tier) 7 TB and up (Full Tier) Working data, lab shared software, results, etc. /oak Oak Storage Working data, lab shared software, results, etc. /tmp no quota, limited by node capacity Per-job temporary files, data read many times during a job Moving Data to/from SCG \u00b6 Globus \u00b6 When moving large amounts of data, either in size, large counts of small files or both, Globus offers an easy to use web interface, a personal connect client that can be installed on your laptop/desktop or on any system you have access to and provides easy access and security using your Stanford login. The Globus tools can be used to move, copy or sync data and will retry in the background on errors. The SCG Globus endpoint is SCG Cluster Storage SCG OnDemand File App \u00b6 The SCG OnDemand File App (https://ondemand.scg.stanford.edu/) offers an intuitive interface to navigate SCG storage and upload or download files. It also includes built in tools to view and edit files in the web browser. Samba \u00b6 The Samba server at samba.scg.stanford.edu presents SCG storage to Stanford campus networks and VPN and makes it possible to easily mount the storage as a shared drive on your local system. Basic instructions/troubleshooting for each major Operating System are below or you can try this direct link if you are feeling lucky and aren\u2019t using Windows: SCG Samba Direct Link Linux \u00b6 Open a terminal and run kinit SUNeTID@stanford.edu replacing SUNetID with your SUNetID. For example, griznog @gambusia : ~ $ kinit griznog @stanford . edu Password for griznog @stanford . edu : griznog @gambusia : ~ $ klist Ticket cache : FILE : / tmp / krb5cc_1000 Default principal : griznog @stanford . edu Valid starting Expires Service principal 07 / 15 / 2019 13 : 08 : 37 07 / 16 / 2019 14 : 08 : 27 krbtgt / stanford . edu @stanford . edu renew until 07 / 22 / 2019 13 : 08 : 27 griznog @gambusia : ~ $ In Linux open Nautilus or your distribution\u2019s file manager app and in the path area or under a menu selection like Connect to server... enter smb://samba.scg.stanford.edu/ and you should be presented with a window displaying the available network shares. The special share labeled with your SUNetID is your SCG $HOME directory. Mac OS X \u00b6 Open a terminal and run kinit SUNeTID@stanford.edu replacing SUNetID with your SUNetID. Open Finder, select Connect to server and enter smb://samba.scg.stanford.edu/ and you should be presented with a Finder window displaying the available network shares. The special share labeled with your SUNetID is your SCG $HOME directory. Windows \u00b6 Open an explore window and in the path section enter \\\\samba.scg.stanford.edu At the login prompt, user your SUNetID and password to connect and you should be presented with a window displaying the available network shares. The special share labeled with your SUNetID is your SCG $HOME directory. If login fails, you may need to run these commands from an Administrator enabled account: ksetup /addkdc stanford.edu krb5auth1.stanford.edu ksetup /addhosttorealmmap .stanford.edu stanford.edu rsync \u00b6 rsync is a time-tested tool for moving data both between remote systems and locally. With a large number of options and features, it\u2019s impossible to completely cover all potential uses of rsync , so a few typical example use cases are shown below. Basic rsync usage is rsync [ options ] [ user@host: ]/ path / to / source [ user@host: ]/ path / to / target Note that only one of source and target can be a remote host. An example of copying files from my local system to SCG is rsync -a -v --partial --progress /mydrive/mydata griznog@login.scg.stanford.edu:/labs/ruthm/ In that command note the following: --partial : If the transfer fails, try to reuse any partially copied files when restarting. -a : A shortcut for archive , pulling in recursion and several other options which attempt to make a complete copy of the source files/directories. --progress : Show a progress bar with transfer speed for each file transferred. /mydrive/mydata : A source directory. Note that including a trailing /, e.g., /home/giznog/mydata/ will cause rsync to work on the contents of the directory rather than the directory itself. This is a subtle difference that can lead to confusion on the target copy. griznog@login.scg.stanford.edu:/labs/ruthm/griznog/ : A target location. The trailing slash on a target has no significance. Some other interesting and useful rsync options are: --delete : Useful when running rsync to update a remote copy where you want to delete any files that have been deleted on the local copy. --remove-source-files : In cases where rsync is being used to quickly clean up data, for instance to reduce usage due to quota, this option will remove files once they have been successfully copied rather than having to wait until the entire rsync completes and deleting them manually. It does not remove directories. --dry-run : Show what rsync would do, but don\u2019t actually do any copy or removal. Useful to test with --delete or --remove-source-files before running a potentially destructive rsync command. sftp \u00b6 sftp provides a secure/encrypted analogs to ftp for any remote sites where ssh access is available. Example usage: griznog @lepomis : ~ $ sftp griznog @login . scg . stanford . edu Connected to login . scg . stanford . edu . sftp > ls Desktop Documents Downloads Logs Music Pictures Projects Public Scratch Templates Videos Working bin myfile ondemand rpmbuild sftp > help Available commands : bye Quit sftp cd path Change remote directory to 'path' chgrp grp path Change group of file 'path' to 'grp' chmod mode path Change permissions of file 'path' to 'mode' chown own path Change owner of file 'path' to 'own' df [ -hi ] [ path ] Display statistics for current directory or filesystem containing 'path' exit Quit sftp get [ -afPpRr ] remote [ local ] Download file reget [ -fPpRr ] remote [ local ] Resume download file reput [ -fPpRr ] [ local ] remote Resume upload file help Display this help text lcd path Change local directory to 'path' lls [ ls-options [path ] ] Display local directory listing lmkdir path Create local directory ln [ -s ] oldpath newpath Link remote file ( - s for symlink ) lpwd Print local working directory ls [ -1afhlnrSt ] [ path ] Display remote directory listing lumask umask Set local umask to 'umask' mkdir path Create remote directory progress Toggle display of progress meter put [ -afPpRr ] local [ remote ] Upload file pwd Display remote working directory quit Quit sftp rename oldpath newpath Rename remote file rm path Delete remote file rmdir path Remove remote directory symlink oldpath newpath Symlink remote file version Show SFTP version ! command Execute 'command' in local shell ! Escape to local shell ? Synonym for help sftp > scp \u00b6 scp provides a secure/encrypted analog to cp which works with remote sources or targets. Example usage: griznog@lepomis:~$ scp myfile griznog@login.scg.stanford.edu: myfile 100% 0 0.0KB/s 00:00 Useful options are -r for recursion (to copy directories) and -v for verbose output. rclone \u00b6 rclone is a powerful file transfer and synchronizing tool that works with a large number of cloud services. Configuration and usage is well documented on the rclone website . One of the most common uses of rclone on SCG is to keep copies of data on SCG archived or backed up to Box or Google Drive.","title":"Managing and Moving Data"},{"location":"tutorials/data_management/#storage-locations","text":"Each user has access to several storage types and locations. The following table lists the storage locations, limits and the use for which each is best suited. Location Quota Usage /home/SUNetID 32 GB Config files, important scripts, private software/data /labs/PI_SUNetID 128 GB (Free Tier) 7 TB and up (Full Tier) Working data, lab shared software, results, etc. /oak Oak Storage Working data, lab shared software, results, etc. /tmp no quota, limited by node capacity Per-job temporary files, data read many times during a job","title":"Storage Locations"},{"location":"tutorials/data_management/#moving-data-tofrom-scg","text":"","title":"Moving Data to/from SCG"},{"location":"tutorials/data_management/#globus","text":"When moving large amounts of data, either in size, large counts of small files or both, Globus offers an easy to use web interface, a personal connect client that can be installed on your laptop/desktop or on any system you have access to and provides easy access and security using your Stanford login. The Globus tools can be used to move, copy or sync data and will retry in the background on errors. The SCG Globus endpoint is SCG Cluster Storage","title":"Globus"},{"location":"tutorials/data_management/#scg-ondemand-file-app","text":"The SCG OnDemand File App (https://ondemand.scg.stanford.edu/) offers an intuitive interface to navigate SCG storage and upload or download files. It also includes built in tools to view and edit files in the web browser.","title":"SCG OnDemand File App"},{"location":"tutorials/data_management/#samba","text":"The Samba server at samba.scg.stanford.edu presents SCG storage to Stanford campus networks and VPN and makes it possible to easily mount the storage as a shared drive on your local system. Basic instructions/troubleshooting for each major Operating System are below or you can try this direct link if you are feeling lucky and aren\u2019t using Windows: SCG Samba Direct Link","title":"Samba"},{"location":"tutorials/data_management/#linux","text":"Open a terminal and run kinit SUNeTID@stanford.edu replacing SUNetID with your SUNetID. For example, griznog @gambusia : ~ $ kinit griznog @stanford . edu Password for griznog @stanford . edu : griznog @gambusia : ~ $ klist Ticket cache : FILE : / tmp / krb5cc_1000 Default principal : griznog @stanford . edu Valid starting Expires Service principal 07 / 15 / 2019 13 : 08 : 37 07 / 16 / 2019 14 : 08 : 27 krbtgt / stanford . edu @stanford . edu renew until 07 / 22 / 2019 13 : 08 : 27 griznog @gambusia : ~ $ In Linux open Nautilus or your distribution\u2019s file manager app and in the path area or under a menu selection like Connect to server... enter smb://samba.scg.stanford.edu/ and you should be presented with a window displaying the available network shares. The special share labeled with your SUNetID is your SCG $HOME directory.","title":"Linux"},{"location":"tutorials/data_management/#mac-os-x","text":"Open a terminal and run kinit SUNeTID@stanford.edu replacing SUNetID with your SUNetID. Open Finder, select Connect to server and enter smb://samba.scg.stanford.edu/ and you should be presented with a Finder window displaying the available network shares. The special share labeled with your SUNetID is your SCG $HOME directory.","title":"Mac OS X"},{"location":"tutorials/data_management/#windows","text":"Open an explore window and in the path section enter \\\\samba.scg.stanford.edu At the login prompt, user your SUNetID and password to connect and you should be presented with a window displaying the available network shares. The special share labeled with your SUNetID is your SCG $HOME directory. If login fails, you may need to run these commands from an Administrator enabled account: ksetup /addkdc stanford.edu krb5auth1.stanford.edu ksetup /addhosttorealmmap .stanford.edu stanford.edu","title":"Windows"},{"location":"tutorials/data_management/#rsync","text":"rsync is a time-tested tool for moving data both between remote systems and locally. With a large number of options and features, it\u2019s impossible to completely cover all potential uses of rsync , so a few typical example use cases are shown below. Basic rsync usage is rsync [ options ] [ user@host: ]/ path / to / source [ user@host: ]/ path / to / target Note that only one of source and target can be a remote host. An example of copying files from my local system to SCG is rsync -a -v --partial --progress /mydrive/mydata griznog@login.scg.stanford.edu:/labs/ruthm/ In that command note the following: --partial : If the transfer fails, try to reuse any partially copied files when restarting. -a : A shortcut for archive , pulling in recursion and several other options which attempt to make a complete copy of the source files/directories. --progress : Show a progress bar with transfer speed for each file transferred. /mydrive/mydata : A source directory. Note that including a trailing /, e.g., /home/giznog/mydata/ will cause rsync to work on the contents of the directory rather than the directory itself. This is a subtle difference that can lead to confusion on the target copy. griznog@login.scg.stanford.edu:/labs/ruthm/griznog/ : A target location. The trailing slash on a target has no significance. Some other interesting and useful rsync options are: --delete : Useful when running rsync to update a remote copy where you want to delete any files that have been deleted on the local copy. --remove-source-files : In cases where rsync is being used to quickly clean up data, for instance to reduce usage due to quota, this option will remove files once they have been successfully copied rather than having to wait until the entire rsync completes and deleting them manually. It does not remove directories. --dry-run : Show what rsync would do, but don\u2019t actually do any copy or removal. Useful to test with --delete or --remove-source-files before running a potentially destructive rsync command.","title":"rsync"},{"location":"tutorials/data_management/#sftp","text":"sftp provides a secure/encrypted analogs to ftp for any remote sites where ssh access is available. Example usage: griznog @lepomis : ~ $ sftp griznog @login . scg . stanford . edu Connected to login . scg . stanford . edu . sftp > ls Desktop Documents Downloads Logs Music Pictures Projects Public Scratch Templates Videos Working bin myfile ondemand rpmbuild sftp > help Available commands : bye Quit sftp cd path Change remote directory to 'path' chgrp grp path Change group of file 'path' to 'grp' chmod mode path Change permissions of file 'path' to 'mode' chown own path Change owner of file 'path' to 'own' df [ -hi ] [ path ] Display statistics for current directory or filesystem containing 'path' exit Quit sftp get [ -afPpRr ] remote [ local ] Download file reget [ -fPpRr ] remote [ local ] Resume download file reput [ -fPpRr ] [ local ] remote Resume upload file help Display this help text lcd path Change local directory to 'path' lls [ ls-options [path ] ] Display local directory listing lmkdir path Create local directory ln [ -s ] oldpath newpath Link remote file ( - s for symlink ) lpwd Print local working directory ls [ -1afhlnrSt ] [ path ] Display remote directory listing lumask umask Set local umask to 'umask' mkdir path Create remote directory progress Toggle display of progress meter put [ -afPpRr ] local [ remote ] Upload file pwd Display remote working directory quit Quit sftp rename oldpath newpath Rename remote file rm path Delete remote file rmdir path Remove remote directory symlink oldpath newpath Symlink remote file version Show SFTP version ! command Execute 'command' in local shell ! Escape to local shell ? Synonym for help sftp >","title":"sftp"},{"location":"tutorials/data_management/#scp","text":"scp provides a secure/encrypted analog to cp which works with remote sources or targets. Example usage: griznog@lepomis:~$ scp myfile griznog@login.scg.stanford.edu: myfile 100% 0 0.0KB/s 00:00 Useful options are -r for recursion (to copy directories) and -v for verbose output.","title":"scp"},{"location":"tutorials/data_management/#rclone","text":"rclone is a powerful file transfer and synchronizing tool that works with a large number of cloud services. Configuration and usage is well documented on the rclone website . One of the most common uses of rclone on SCG is to keep copies of data on SCG archived or backed up to Box or Google Drive.","title":"rclone"},{"location":"tutorials/gpus/","text":"Available GPUs \u00b6 GPUs are available in two locations: All login nodes are equipped with a small GPU to allow compiling and testing GPU applications. The UV300 has 4 x nVidia Tesla P100 GPUs. Requesting GPUs \u00b6 Job requests for GPUs need to specify at least the following three Slurm options: An account with access to the nih_s10 partition, usually one of: --account=your_PI_SUNetID --account=your_Project_ID The nih_s10 partition: --partition=nih_s10 For N gpus (1 <= N <= 4) : --gres=gpu:N CUDA \u00b6 There are several versions of CUDA available to use, for a complete list run: module avail cuda There are also several CUDA enabled python virtual environments, to see a list run: module load anaconda; conda env list | grep cuda","title":"Using GPUs"},{"location":"tutorials/gpus/#available-gpus","text":"GPUs are available in two locations: All login nodes are equipped with a small GPU to allow compiling and testing GPU applications. The UV300 has 4 x nVidia Tesla P100 GPUs.","title":"Available GPUs"},{"location":"tutorials/gpus/#requesting-gpus","text":"Job requests for GPUs need to specify at least the following three Slurm options: An account with access to the nih_s10 partition, usually one of: --account=your_PI_SUNetID --account=your_Project_ID The nih_s10 partition: --partition=nih_s10 For N gpus (1 <= N <= 4) : --gres=gpu:N","title":"Requesting GPUs"},{"location":"tutorials/gpus/#cuda","text":"There are several versions of CUDA available to use, for a complete list run: module avail cuda There are also several CUDA enabled python virtual environments, to see a list run: module load anaconda; conda env list | grep cuda","title":"CUDA"},{"location":"tutorials/job_scripts/","text":"Why use Slurm? \u00b6 Within the SCG cluster, Slurm functions as a resource broker. Submitting a job to Slurm requests a set of CPU and memory resources. Slurm orders these requests and gives them a priority based on the cluster configuration and runs each job on the most appropriate available resource in the order that respects the job priority or, when possible, squeezes in short jobs via a backfill scheduler to harvest unused cpu time. Using Slurm allows many users to fairly share a set of computational resources with greatly reduced danger of negatively impacting anotehr persons jobs or work while also allowing the resources to be better and more fully utilized over time. In addition to making it easier to effectively share a resource, Slurm also acts as a powerful tool for managing workflows. By encapsulating steps in job scripts it becomes possible to easily repeat and reuse workflows and having such job scripts adds to the documentation of the associated process. What is a job script? \u00b6 A job script is a normal unix shell script which optionally contains lines flagged such that the Slurm submission process can read arguments from them. A simple hello world job script using Bash would be 1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/bash # See `man sbatch` or https://slurm.schedmd.com/sbatch.html for descriptions # of sbatch options. #SBATCH --job-name=hello_world #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --partition=interactive #SBATCH --account=default #SBATCH --time=1:00:00 echo 'Hello World!' Submitting a job \u00b6 Saving the script as hello_world.sh and submitting it to Slurm with sbatch results in the job running and producing an output file. The default output file is slurm-JOB_ID.out located in the directory from which the job was submitted. For example: [griznog@smsx10srw-srcf-d15-37 jobs]$ sbatch hello_world.sh Submitted batch job 6592914 [griznog@smsx10srw-srcf-d15-37 jobs]$ cat slurm-6592914.out Hello World! The sbatch man page lists all sbatch options. Managing Slurm Jobs \u00b6 squeue \u00b6 Once a job is submitted, it either immediately runs if resources are available and there are no jobs ahead of it in the queue or it is queued and marked as Pending. Pending and running jobs can be monitored with the squeue command. The default squeue output shows all jobs: [griznog@smsx10srw-srcf-d15-37 jobs]$ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 6564781 interacti wrap crolle PD 0:00 1 (Resources) 6564782 interacti wrap crolle PD 0:00 1 (Priority) 6564783 interacti wrap crolle PD 0:00 1 (Priority) 6564784 interacti wrap crolle PD 0:00 1 (Priority) 6564785 interacti wrap crolle PD 0:00 1 (Priority) 6564786 interacti wrap crolle PD 0:00 1 (Priority) 6564787 interacti wrap crolle PD 0:00 1 (Priority) 6564788 interacti wrap crolle PD 0:00 1 (Priority) 6564789 interacti wrap crolle PD 0:00 1 (Priority) 6564790 interacti wrap crolle PD 0:00 1 (Priority) 6564791 interacti wrap crolle PD 0:00 1 (Priority) 6564792 interacti wrap crolle PD 0:00 1 (Priority) 6592902 dtn seqctr_s root PD 0:00 1 (BeginTime) 6564793 interacti wrap crolle PD 0:00 1 (Priority) 6564794 interacti wrap crolle PD 0:00 1 (Priority) 6564795 interacti wrap crolle PD 0:00 1 (Priority) ... Some useful squeue commands are: Command Description squeue -u $USER Show only jobs owned by $USER. squeue -t pd Show only pending jobs. -t r to show only running jobs\u2019 squeue -j JOBID Show details for job JOBID. squeue -j JOBID --format=\"%m\" Use --format to show only the memory requested. scancel \u00b6 The simplest method of cancelling a running job or job array task is to use scancel . For example: [griznog@smsx10srw-srcf-d15-37 jobs]$ squeue -u $USER JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 6593409 interacti wrap griznog PD 0:00 1 (Priority) [griznog@smsx10srw-srcf-d15-37 jobs]$ scancel 6593409 [griznog@smsx10srw-srcf-d15-37 jobs]$ squeue -u $USER JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) [griznog@smsx10srw-srcf-d15-37 jobs]$ The scancel man page lists the options to scancel , many of which are useful for selecting subsets of jobs to operate on. scancel can also be used to send specific signals to a jobs processes. This can be useful for more advanced jobs which are designed to perform different functions in response to receiving a signal, for instance, applications that can perform a checkpoint might be triggered to do so manually with a signal sent via scancel . sinfo \u00b6 The sinfo command can be used to list available partitions, show status and list node and partition configurations. To list partitions: [griznog@smsx10srw-srcf-d15-37 jobs]$ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST batch* up 120-00:00: 22 mix dper730xd-srcf-d16-[01,03,05,07,09,11,13,15],dper7425-srcf-d15-13,sgisummit-frcf-111-[08,12,14,16,18,20,24,26,28,34,36,38],sgiuv20-rcf-111-32 batch* up 120-00:00: 3 alloc sgisummit-frcf-111-[10,22,30] batch* up 120-00:00: 24 idle dper730xd-srcf-d16-[17,19,21,23,25,27,29,31,33,35,37,39],dper930-srcf-d15-05,dper7425-srcf-d15-[09,11,15,17,19,21,23,25,27,29,31] interactive up 120-00:00: 2 drain* hppsl230s-rcf-412-01-l,hppsl230s-rcf-412-02-l interactive up 120-00:00: 5 mix dper910-rcf-412-20,hppsl230s-rcf-412-01-r,hppsl230s-rcf-412-02-r,hppsl230s-rcf-412-03-l,hppsl230s-rcf-412-03-r interactive up 120-00:00: 18 idle hppsl230s-rcf-412-04-l,hppsl230s-rcf-412-04-r,hppsl230s-rcf-412-05-l,hppsl230s-rcf-412-05-r,hppsl230s-rcf-412-06-l,hppsl230s-rcf-412-06-r,hppsl230s-rcf-412-07-l,hppsl230s-rcf-412-07-r,hppsl230s-rcf-412-08-l,hppsl230s-rcf-412-08-r,hppsl230s-rcf-412-09-l,hppsl230s-rcf-412-09-r,hppsl230s-rcf-412-10-l,hppsl230s-rcf-412-10-r,hppsl230s-rcf-412-11-l,hppsl230s-rcf-412-11-r,hppsl230s-rcf-412-12-l,hppsl230s-rcf-412-12-r nih_s10 up 4-00:00:00 1 mix sgiuv300-srcf-d10-01 nih_s10_gpu up 4-00:00:00 1 mix sgiuv300-srcf-d10-01 dtn up 120-00:00: 2 idle cfxs2600gz-rcf-114-[06,08] apps up 120-00:00: 1 idle dper7425-srcf-d10-37 sacct \u00b6 The sacct command can be used to retrieve infomration about jobs that have completed. For example: [griznog@smsx10srw-srcf-d15-37 jobs]$ sacct -j 6593409 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 6593409 wrap interacti+ default 1 CANCELLED+ 0:0 6593409.bat+ batch default 1 CANCELLED 0:15 6593409.ext+ extern default 1 COMPLETED 0:0 When troubleshooting jobs, the ExitCode can be useful to determine how the job ended as it shows the exit code of the job script and the signal which caused the process to terminate. For the example above, the job was cancelled and it\u2019s script was killed by signal 15 with the job script returning 0. The sacct command can return many details about jobs, consult the sacct man page for more information about options and values that can be retrieved for jobs. More information \u00b6 For more information about additional Slurm commands, see the Slurm documentation .","title":"Working with Slurm"},{"location":"tutorials/job_scripts/#why-use-slurm","text":"Within the SCG cluster, Slurm functions as a resource broker. Submitting a job to Slurm requests a set of CPU and memory resources. Slurm orders these requests and gives them a priority based on the cluster configuration and runs each job on the most appropriate available resource in the order that respects the job priority or, when possible, squeezes in short jobs via a backfill scheduler to harvest unused cpu time. Using Slurm allows many users to fairly share a set of computational resources with greatly reduced danger of negatively impacting anotehr persons jobs or work while also allowing the resources to be better and more fully utilized over time. In addition to making it easier to effectively share a resource, Slurm also acts as a powerful tool for managing workflows. By encapsulating steps in job scripts it becomes possible to easily repeat and reuse workflows and having such job scripts adds to the documentation of the associated process.","title":"Why use Slurm?"},{"location":"tutorials/job_scripts/#what-is-a-job-script","text":"A job script is a normal unix shell script which optionally contains lines flagged such that the Slurm submission process can read arguments from them. A simple hello world job script using Bash would be 1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/bash # See `man sbatch` or https://slurm.schedmd.com/sbatch.html for descriptions # of sbatch options. #SBATCH --job-name=hello_world #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --partition=interactive #SBATCH --account=default #SBATCH --time=1:00:00 echo 'Hello World!'","title":"What is a job script?"},{"location":"tutorials/job_scripts/#submitting-a-job","text":"Saving the script as hello_world.sh and submitting it to Slurm with sbatch results in the job running and producing an output file. The default output file is slurm-JOB_ID.out located in the directory from which the job was submitted. For example: [griznog@smsx10srw-srcf-d15-37 jobs]$ sbatch hello_world.sh Submitted batch job 6592914 [griznog@smsx10srw-srcf-d15-37 jobs]$ cat slurm-6592914.out Hello World! The sbatch man page lists all sbatch options.","title":"Submitting a job"},{"location":"tutorials/job_scripts/#managing-slurm-jobs","text":"","title":"Managing Slurm Jobs"},{"location":"tutorials/job_scripts/#squeue","text":"Once a job is submitted, it either immediately runs if resources are available and there are no jobs ahead of it in the queue or it is queued and marked as Pending. Pending and running jobs can be monitored with the squeue command. The default squeue output shows all jobs: [griznog@smsx10srw-srcf-d15-37 jobs]$ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 6564781 interacti wrap crolle PD 0:00 1 (Resources) 6564782 interacti wrap crolle PD 0:00 1 (Priority) 6564783 interacti wrap crolle PD 0:00 1 (Priority) 6564784 interacti wrap crolle PD 0:00 1 (Priority) 6564785 interacti wrap crolle PD 0:00 1 (Priority) 6564786 interacti wrap crolle PD 0:00 1 (Priority) 6564787 interacti wrap crolle PD 0:00 1 (Priority) 6564788 interacti wrap crolle PD 0:00 1 (Priority) 6564789 interacti wrap crolle PD 0:00 1 (Priority) 6564790 interacti wrap crolle PD 0:00 1 (Priority) 6564791 interacti wrap crolle PD 0:00 1 (Priority) 6564792 interacti wrap crolle PD 0:00 1 (Priority) 6592902 dtn seqctr_s root PD 0:00 1 (BeginTime) 6564793 interacti wrap crolle PD 0:00 1 (Priority) 6564794 interacti wrap crolle PD 0:00 1 (Priority) 6564795 interacti wrap crolle PD 0:00 1 (Priority) ... Some useful squeue commands are: Command Description squeue -u $USER Show only jobs owned by $USER. squeue -t pd Show only pending jobs. -t r to show only running jobs\u2019 squeue -j JOBID Show details for job JOBID. squeue -j JOBID --format=\"%m\" Use --format to show only the memory requested.","title":"squeue"},{"location":"tutorials/job_scripts/#scancel","text":"The simplest method of cancelling a running job or job array task is to use scancel . For example: [griznog@smsx10srw-srcf-d15-37 jobs]$ squeue -u $USER JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 6593409 interacti wrap griznog PD 0:00 1 (Priority) [griznog@smsx10srw-srcf-d15-37 jobs]$ scancel 6593409 [griznog@smsx10srw-srcf-d15-37 jobs]$ squeue -u $USER JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) [griznog@smsx10srw-srcf-d15-37 jobs]$ The scancel man page lists the options to scancel , many of which are useful for selecting subsets of jobs to operate on. scancel can also be used to send specific signals to a jobs processes. This can be useful for more advanced jobs which are designed to perform different functions in response to receiving a signal, for instance, applications that can perform a checkpoint might be triggered to do so manually with a signal sent via scancel .","title":"scancel"},{"location":"tutorials/job_scripts/#sinfo","text":"The sinfo command can be used to list available partitions, show status and list node and partition configurations. To list partitions: [griznog@smsx10srw-srcf-d15-37 jobs]$ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST batch* up 120-00:00: 22 mix dper730xd-srcf-d16-[01,03,05,07,09,11,13,15],dper7425-srcf-d15-13,sgisummit-frcf-111-[08,12,14,16,18,20,24,26,28,34,36,38],sgiuv20-rcf-111-32 batch* up 120-00:00: 3 alloc sgisummit-frcf-111-[10,22,30] batch* up 120-00:00: 24 idle dper730xd-srcf-d16-[17,19,21,23,25,27,29,31,33,35,37,39],dper930-srcf-d15-05,dper7425-srcf-d15-[09,11,15,17,19,21,23,25,27,29,31] interactive up 120-00:00: 2 drain* hppsl230s-rcf-412-01-l,hppsl230s-rcf-412-02-l interactive up 120-00:00: 5 mix dper910-rcf-412-20,hppsl230s-rcf-412-01-r,hppsl230s-rcf-412-02-r,hppsl230s-rcf-412-03-l,hppsl230s-rcf-412-03-r interactive up 120-00:00: 18 idle hppsl230s-rcf-412-04-l,hppsl230s-rcf-412-04-r,hppsl230s-rcf-412-05-l,hppsl230s-rcf-412-05-r,hppsl230s-rcf-412-06-l,hppsl230s-rcf-412-06-r,hppsl230s-rcf-412-07-l,hppsl230s-rcf-412-07-r,hppsl230s-rcf-412-08-l,hppsl230s-rcf-412-08-r,hppsl230s-rcf-412-09-l,hppsl230s-rcf-412-09-r,hppsl230s-rcf-412-10-l,hppsl230s-rcf-412-10-r,hppsl230s-rcf-412-11-l,hppsl230s-rcf-412-11-r,hppsl230s-rcf-412-12-l,hppsl230s-rcf-412-12-r nih_s10 up 4-00:00:00 1 mix sgiuv300-srcf-d10-01 nih_s10_gpu up 4-00:00:00 1 mix sgiuv300-srcf-d10-01 dtn up 120-00:00: 2 idle cfxs2600gz-rcf-114-[06,08] apps up 120-00:00: 1 idle dper7425-srcf-d10-37","title":"sinfo"},{"location":"tutorials/job_scripts/#sacct","text":"The sacct command can be used to retrieve infomration about jobs that have completed. For example: [griznog@smsx10srw-srcf-d15-37 jobs]$ sacct -j 6593409 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 6593409 wrap interacti+ default 1 CANCELLED+ 0:0 6593409.bat+ batch default 1 CANCELLED 0:15 6593409.ext+ extern default 1 COMPLETED 0:0 When troubleshooting jobs, the ExitCode can be useful to determine how the job ended as it shows the exit code of the job script and the signal which caused the process to terminate. For the example above, the job was cancelled and it\u2019s script was killed by signal 15 with the job script returning 0. The sacct command can return many details about jobs, consult the sacct man page for more information about options and values that can be retrieved for jobs.","title":"sacct"},{"location":"tutorials/job_scripts/#more-information","text":"For more information about additional Slurm commands, see the Slurm documentation .","title":"More information"},{"location":"tutorials/ssh_controlmaster/","text":"Why use ControlMaster ? \u00b6 Two-factor auth is painful. Secure? Yes. Necessary? Yes. But, painful. Using the ssh config option ControlMaster can ease the pain a bit if you like to work with multiple SSH sessions to the login nodes. What is ControlMaster ? \u00b6 ControlMaster is an ssh client option that will create a socket file which allows the initial ssh connection to a host be reused and optionally persist after the initial session has disconnected. In practice this means the first connection to a host will be authenticated normally, with password, GSSAPI, two-factor, etc., as required, but subsequent connections will simply reuse the initial connection without requiring authentication. ~/.ssh/config \u00b6 Add the following to end of ~/.ssh/config , if that file doesn\u2019t exist then create it with permissions 0600, e.g., touch ~/.ssh/config; chmod 0600 ~/.ssh/config . Host * # Have ControlMaster do the Right Thing(tm) ControlMaster auto # Put the socket file here. Optionally create a directory # for these and adjust this accordingly. ControlPath ~/.ssh/%r@%h:%p # If the initial and all other sessions exit, keep the # hold controlmaster open for 300 seconds before closing. ControlPersist 300s Test \u00b6 Once that is in place, ssh to a host, open a second terminal and ssh to the same host and it should connect without prompting for authentication.","title":"Avoiding Duo with SSH"},{"location":"tutorials/ssh_controlmaster/#why-use-controlmaster","text":"Two-factor auth is painful. Secure? Yes. Necessary? Yes. But, painful. Using the ssh config option ControlMaster can ease the pain a bit if you like to work with multiple SSH sessions to the login nodes.","title":"Why use ControlMaster?"},{"location":"tutorials/ssh_controlmaster/#what-is-controlmaster","text":"ControlMaster is an ssh client option that will create a socket file which allows the initial ssh connection to a host be reused and optionally persist after the initial session has disconnected. In practice this means the first connection to a host will be authenticated normally, with password, GSSAPI, two-factor, etc., as required, but subsequent connections will simply reuse the initial connection without requiring authentication.","title":"What is ControlMaster?"},{"location":"tutorials/ssh_controlmaster/#sshconfig","text":"Add the following to end of ~/.ssh/config , if that file doesn\u2019t exist then create it with permissions 0600, e.g., touch ~/.ssh/config; chmod 0600 ~/.ssh/config . Host * # Have ControlMaster do the Right Thing(tm) ControlMaster auto # Put the socket file here. Optionally create a directory # for these and adjust this accordingly. ControlPath ~/.ssh/%r@%h:%p # If the initial and all other sessions exit, keep the # hold controlmaster open for 300 seconds before closing. ControlPersist 300s","title":"~/.ssh/config"},{"location":"tutorials/ssh_controlmaster/#test","text":"Once that is in place, ssh to a host, open a second terminal and ssh to the same host and it should connect without prompting for authentication.","title":"Test"}]}